spring:
  profiles: ${METATRON_ENV_MODE:local}
  http:
    multipart: # Used to adjust the maximum size when uploading files
      max-file-size: 100Mb # Size of upload chunks (Has impact on displayed progress in fe)
  datasource:
    platform: h2
    driver-class-name: org.h2.Driver
    url: jdbc:h2:file:/opt/bdl/data/discovery/h2db/polaris;AUTO_SERVER=TRUE;DB_CLOSE_ON_EXIT=FALSE
    username: sa
    password: sa
    max-active: 10
server:
  port: 8180
logging:
  config: classpath:logback-console.xml

polaris:
  storage:
    s3:
      bucket: dataprep-${TENANT_ID}
      region: default
      only: true
    # The stagedb concept is internally used as an intermediary processing medium when processing large amounts of data
    # when ingesting or transforming data in an application. We are using hive and specify the information related to it.
    stagedb:
      hostname: localhost   # for hiveserver
      port: 10000           # for hiveserver
      username: hive        # for hiveserver
      password: hive        # for hiveserver
      metastore:            # specify additional information when you want to get more information such as partitions.
        uri: thrift://localhost:9083
        jdbc:
          url: jdbc:mysql://localhost:3306/hive
          username: hive
          password: hive

  engine:
    # Specify representative access url by engine node, application requires only 3 nodes
    hostname:
      broker: http://discovery-${TENANT_ID}-druid:8082
      overlord: http://discovery-${TENANT_ID}-druid:8090
      coordinator: http://discovery-${TENANT_ID}-druid:8081
      historical: http://discovery-${TENANT_ID}-druid:8083
      middleManager: http://discovery-${TENANT_ID}-druid:8091
    ingestion:
      baseDir: /opt/bdl/ingest  # This will be passed to the Druid task to look in this folder on the Druid container
      # Used when forwarding files from an application to an engine cluster when loading local files
      loader:
        remoteType: SHARED  # "LOCAL" is the default value, but if the engine is configured as a separate cluster, use "SSH" mode for remote communication.
        remoteDir: /opt/bdl/ingest # we will copy the ingest file here
    query:  # Unlike in "ingestion", when downloading query results, it is used to transfer download file generated by engine to application
      localResultDir: ${java.io.tmpdir:-/tmp}
      defaultForwardUrl: local:/tmp

  datasource:
    ingestion:
      retries:  # Retry properties used when checking the status of engine when ingestion
        delay: 3
        maxDelay: 90
        maxDuration : 3600

  workbench:
    defaultResultSize: 1000        # Row number of results to display by default on the screen
    maxResultSize: 1000000         # The maximum row number of results that a user can specify
    maxFetchSize: 1000             # Fetch size when loading sql results, only applies to supported db library
    tempCSVPath: /tmp              # Specify a directory to temporarily store query results

  #Set up Hive related information when using Data Prep.
  dataprep:
    localBaseDir: /opt/bdl/data/discovery/dataprep    # Where uploaded files, local file snapshots, serialized previews are stored
    hadoopConfDir: /opt/bdl/etc/hadoop                # Set $HADOOP_CONF_DIR
    hadoopUser: bdl                                    # user to connect to HDFS
    stagingBaseDir: hdfs://bdlcluster.bdl.cloud:19998/${TENANT_ID}/dataprep   # Where uploaded files, HDFS file snapshots, files for Hive external tables are stored
    s3BaseDir: s3a://dataprep-${TENANT_ID}
    sampling:
      timeout: 20 # Timeout for rule edit sessions
      limit-rows: 10000
      cores: 1
    etl:
      timeout: 360000  # Timeout for generating snapshots
      limit-rows: 1000000000
      cores: 4
      spark:
        port: 5300
        master: spark://dproc-${TENANT_ID}-master:7077

spark:
  driver-host: discovery-${TENANT_ID}-discovery

sso:
  enabled: true
  forceHttps: true
